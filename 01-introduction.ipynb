{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9073874,"sourceType":"datasetVersion","datasetId":5473557}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n!pip install boto3 --upgrade\n!git clone https://github.com/nlp-with-transformers/notebooks.git\n%cd notebooks\nfrom install import *\ninstall_requirements()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T13:21:19.040740Z","iopub.execute_input":"2024-07-31T13:21:19.041168Z","iopub.status.idle":"2024-07-31T13:21:37.115702Z","shell.execute_reply.started":"2024-07-31T13:21:19.041130Z","shell.execute_reply":"2024-07-31T13:21:37.114486Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: destination path 'notebooks' already exists and is not an empty directory.\n/kaggle/working/notebooks\n‚è≥ Installing base requirements ...\n‚úÖ Base requirements installed!\n‚è≥ Installing Git LFS ...\n‚úÖ Git LFS installed!\n","output_type":"stream"}]},{"cell_type":"code","source":"# hide\nfrom utils import *\nsetup_chapter()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T13:22:26.324601Z","iopub.execute_input":"2024-07-31T13:22:26.325031Z","iopub.status.idle":"2024-07-31T13:22:43.300729Z","shell.execute_reply.started":"2024-07-31T13:22:26.324995Z","shell.execute_reply":"2024-07-31T13:22:43.299609Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.34.151)\nRequirement already satisfied: botocore<1.35.0,>=1.34.151 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.34.151)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.151->boto3) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.151->boto3) (1.26.18)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.151->boto3) (1.16.0)\nNo GPU was detected! This notebook can be *very* slow without a GPU üê¢\nUsing transformers v4.16.2\nUsing datasets v1.16.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hello Transformers","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"transformer-timeline\" caption=\"The transformers timeline\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_timeline.png?raw=true\" id=\"transformer-timeline\"/>","metadata":{}},{"cell_type":"markdown","source":"## The Encoder-Decoder Framework","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"rnn\" caption=\"Unrolling an RNN in time.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_rnn.png?raw=true\" id=\"rnn\"/>","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"enc-dec\" caption=\"Encoder-decoder architecture with a pair of RNNs. In general, there are many more recurrent layers than those shown.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_enc-dec.png?raw=true\" id=\"enc-dec\"/>","metadata":{}},{"cell_type":"markdown","source":"## Attention Mechanisms","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"enc-dec-attn\" caption=\"Encoder-decoder architecture with an attention mechanism for a pair of RNNs.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_enc-dec-attn.png?raw=true\" id=\"enc-dec-attn\"/> ","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"attention-alignment\" width=\"500\" caption=\"RNN encoder-decoder alignment of words in English and the generated translation in French (courtesy of Dzmitry Bahdanau).\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter02_attention-alignment.png?raw=true\" id=\"attention-alignment\"/> ","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"transformer-self-attn\" caption=\"Encoder-decoder architecture of the original Transformer.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_self-attention.png?raw=true\" id=\"transformer-self-attn\"/> ","metadata":{}},{"cell_type":"markdown","source":"## Transfer Learning in NLP","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"transfer-learning\" caption=\"Comparison of traditional supervised learning (left) and transfer learning (right).\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_transfer-learning.png?raw=true\" id=\"transfer-learning\"/>  ","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"ulmfit\" width=\"500\" caption=\"The ULMFiT process (courtesy of Jeremy Howard).\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_ulmfit.png?raw=true\" id=\"ulmfit\"/>","metadata":{}},{"cell_type":"markdown","source":"## Hugging Face Transformers: Bridging the Gap","metadata":{}},{"cell_type":"markdown","source":"## A Tour of Transformer Applications","metadata":{}},{"cell_type":"code","source":"text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\nfrom your online store in Germany. Unfortunately, when I opened the package, \\\nI discovered to my horror that I had been sent an action figure of Megatron \\\ninstead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\ndilemma. To resolve the issue, I demand an exchange of Megatron for the \\\nOptimus Prime figure I ordered. Enclosed are copies of my records concerning \\\nthis purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Classification","metadata":{}},{"cell_type":"code","source":"#hide_output\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\noutputs = classifier(text)\npd.DataFrame(outputs)    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Named Entity Recognition","metadata":{}},{"cell_type":"code","source":"ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\noutputs = ner_tagger(text)\npd.DataFrame(outputs)    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question Answering ","metadata":{}},{"cell_type":"code","source":"reader = pipeline(\"question-answering\")\nquestion = \"What does the customer want?\"\noutputs = reader(question=question, context=text)\npd.DataFrame([outputs])    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summarization","metadata":{}},{"cell_type":"code","source":"summarizer = pipeline(\"summarization\")\noutputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\nprint(outputs[0]['summary_text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Translation","metadata":{}},{"cell_type":"code","source":"translator = pipeline(\"translation_en_to_de\", \n                      model=\"Helsinki-NLP/opus-mt-en-de\")\noutputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\nprint(outputs[0]['translation_text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Generation","metadata":{}},{"cell_type":"code","source":"#hide\nfrom transformers import set_seed\nset_seed(42) # Set the seed to get reproducible results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = pipeline(\"text-generation\")\nresponse = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt = text + \"\\n\\nCustomer service response:\\n\" + response\noutputs = generator(prompt, max_length=200)\nprint(outputs[0]['generated_text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Hugging Face Ecosystem","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"ecosystem\" width=\"500\" caption=\"An overview of the Hugging Face ecosystem of libraries and the Hub.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_hf-ecosystem.png?raw=true\" id=\"ecosystem\"/>","metadata":{}},{"cell_type":"markdown","source":"### The Hugging Face Hub","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"hub-overview\" width=\"1000\" caption=\"The models page of the Hugging Face Hub, showing filters on the left and a list of models on the right.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_hub-overview.png?raw=true\" id=\"hub-overview\"/> ","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"hub-model-card\" width=\"1000\" caption=\"A example model card from the Hugging Face Hub. The inference widget is shown on the right, where you can interact with the model.\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter01_hub-model-card.png?raw=true\" id=\"hub-model-card\"/> ","metadata":{}},{"cell_type":"markdown","source":"### Hugging Face Tokenizers","metadata":{}},{"cell_type":"markdown","source":"### Hugging Face Datasets","metadata":{}},{"cell_type":"markdown","source":"### Hugging Face Accelerate","metadata":{}},{"cell_type":"markdown","source":"## Main Challenges with Transformers","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}}]}